# Real-Time Streaming Analytics pipeline with Kinesis, Flink and OpenSearch

Core idea:-    

**We've integrated the primary phases -->  data ingestion, processing, storage & visualisation --> into a single cohesive pipeline**.    
and have utilised **Kinesis, Lambda, Glue & OpenSearch.**

</br>

Our point of emphasis:-   

‚û°Ô∏è We've **optimised for non-functional aspects &rarr; scalability and performance throughout.**

</br>


## Project Workflow 

![Real-Time Streaming Analytics with Kinesis (1)](https://github.com/TanishkaMarrott/Real-Time-Streaming-Analytics-with-Kinesis-Flink-and-OpenSearch/assets/78227704/99edb176-7c2f-485d-bae0-b32629942201)


</br>

## Specifics into the data ingestion Layer

**We've utilised Kinesis Data Streams** for capturing and storing real-time streaming data    

> Please make sure to check out this file --> `EnhancedProducer.java`

</br>

### The Producer Workflow

We'll first create the Kinesis producer configuration                                     
This is where we'll specify the parameters like timeout, maxConnections, etc.                     
‚¨áÔ∏è                     
We will then initialize a Kinesis producer instance** with the said configurations.                     
‚¨áÔ∏è                     
Extract the data from the telemetry CSV we've provided.                     
Each row in the CSV will then be converted into a Trip object.                     
‚¨áÔ∏è                     
We'll then set up an Executor Service.                       
Helps us in sending data concurrently through multiple threads, improving throughput.                     
‚¨áÔ∏è                            
We will then send to our stream asynchronously using CompletableFuture.                     
‚¨áÔ∏è                     
Will check if our submission was successful and log shard ID or error, as may be the case.              
We'll shut down the Executor Service and Kinesis Producer gracefully, while ensuring that all our tasks are completed without abrupt termination.              

</br>

## What sort of design decisions did we make for the ingestion layer?

### A &rarr; _We opted for the _On-demand capacity mode_ for KDS:-_
           
--> Our data stream _must scale automatically_ whenever there're variations in the workload.

> **We do not need to manually handle shard capacities, it'll automatically scale based on the influx of streaming data** üëç

</br>

### B &rarr; Had to optimize on the thread management mechanism

#### **Approach I** - wherein we exclusively used ExecutorService

--> **Initially, we utilized _only_ `ExecutorService` to manage our thread **
NOTE:- _This setup created a partially asynchronous workflow._

Why partially asynchronous?
While ExecutorService enables concurrent execution, allowing multiple tasks or threads to run in parallel, it only manages the submission of these tasks asynchronously. The retrieval of results, on the other hand, involves a blocking operation.

> _Thus, while task submission occurs asynchronously, retrieving the results using the Future object does not._

</br>

> **Potential Red Flag** üö©: The `Future.get()` method compels the calling thread to wait until the task completes which means we'e blocking the thread.
> NOT RECOMMENDED. 

</br>

### How did we overcome this challenge then?

We had to quickly transform our approach. We anyhow had to get a fully asynchronuos workflow for sending data to Kinesis :

**--> _Integrated `CompletableFuture` + `ExecutorService`_**
            
KEY POINTS TO NOTE:- 

_Point 1 :-_ `ExecutorService` will only be responsible for managing the thread pool --> This only takes care of the concurrency aspect                  

_Point 2:-_ **Now, that we've combined `CompletableFuture`, means we're NOT blocking any thread, it can perform other operations without waiting for task completion** üí°

</br>

> **What did we achieve ?**
>     
> **My entire workflow is now fully asynchronous. ‚Ü™Ô∏è Operational efficient because we've now improved throughput** üëç

</br>

### C &rarr; Dynamically sized thread pool 

A couple of reasons here:-

--> **Ours is more of a hybrid workload. ‚û°Ô∏è It's a mix of CPU-Bound and I/O bound threads.** 
(It involves both computations as well as sending data to Kinesis)

In such a scenario, **I'll advise to go with a factor of 2 (2 * the number of available cores)**

> #### Why did we go with such a heuristic? (2 * the number of CPU Cores)
>
> _Simple Answer:-_
> **A balanced resource utilisation** üí°

</br>

1 -->  **We're actively engaging all the CPU cores, without overwhelming the system. Each CPU would have two threads to work on, the CPU-bound, and the I/O Bound .**
Once the I/O bound threads wait for the operations to complete, the cpu could then proceed with the computational operations.

2 --> We're cognizant of the resources we're using --> **There should neither be underutilisation or over-allocation.** ‚úîÔ∏è üèÅ

</br>

> So, **irrespective of our environments, our application can quickly adapt to machines, making our application responsive and scalable from the get-go**
>
> This is **one of my strategy I often use whenever we're trying to optimize the software architecture itself to make it way more resource efficient plus scalable üëç.**

</br>

3 --> **We had to save on the infra-costs as well**, We're working on the cloud, wherein we'd be charged based on the number of running threads. **We do not want too many threads competing for CPU Time --> (We do not want too much context-switching)** Neither do we want too less threads means we aren't performant enough

‚û°Ô∏è Resource efficient + Performance optimised üëç

</br>

### D --> We've implemented a retry + progressive backoff mechanism 


1 --> We were adamant on implementing some sort of error handling mechanisms:-
>
> _Point 1_ --> Something that assures us that **despite of temporary setbacks or transient errors, our application will still be well-equipped to run reliably**
>
  ‚û°Ô∏è **We'll maintain a good level of Operational stability + Service continuity üëç**
>
> </br>
>
> _Point 2_ --> **Plus a backoff mechanism in place, that progressively increases the time interval between two successive retries**                                 
> We're basically achieving 3 things here:-  
>
>     A - We're minimizing system workload, we aren't overwhelming our resources                                
>     B - We're making our application stable --> We'll limit the number of retries allowed, so, even in face of errors, our application would operate reliably (we do not want it to enter into a loop of infinite failures) 
>     C - We end up improvising the data consistency and processing, handling errors GRACEFULLY ‚û°Ô∏è We're giving errors more time to resolve, by increasing the time interval between two subsequent retries


More so, it's a predictable system behaviour, We have a well-defined retry policy with exponential backoff.

**What did we achieve ? Strong availability + reliability** ‚úÖ

</br>


## Data transformation layer for this architecture

Services we've utilised :- **Kinesis Data Firehose + Glue**

### What was our rationale behind using firehose plus glue?
                      
**We've used glue as a central metadata repository** through data catalog.               
‚û°Ô∏è **Athena can then use this schema information for quering data in s3**. 

> Had we used firehose by itself, it would just aid in loading streaming data into S3. &rarr; **The definitions we've stored in glue _actually_ enhance Athena's querying capabilities** 

> I've shared the Table Definition above, firehose references this definition in glue

</br>

## Data Transformations using lambda

### considerations before processing in lambda

1 --> We had to weigh in the impact on downstream systems plus the user experience. This means the processing logic on Lambda shouldn't be too heavy. 
2 --> Plus if our volume of data and frequency of data processing requests are too high, lambda might start getting strained, especially if we're using a lot of lambda's memory or getting too close to the 15 minute cap on Lambda's execution.
3--> 



&#8594; Designed to processes streaming data, focusing on data transformation and standardisation. Sets up logging for monitoring, **_converts pickupDate and dropoffDate fields to ISO 8601 format._** Having decoded the records from base-64, it **_inserts the source 'NYCTAXI' column._**
Function has been designed to handle errors, generating responses for each processed record, and manages batch processing as well.

</br>

## Design decisions we've made in the transformation layer

### _A -->**Converting the source record format:-**_

 We've used **Kinesis Data Firehose** for  data delivery into S3 & some initial data transformation.

 #### Why did we convert the format from JSON to Parquet? 
 
 A couple of reasons here:-
 
 **--> It helps us reduce storage costs significantly**.  
 
 **--> We were looking for an efficient kind of query mechanism for Athena**. And **Parquet's columnar structure** works very well.

</br>
  
### _B --> Optimising the buffer size & interval:-_

**We had to maximize the buffer interval time for data delivery into S3.**

</br>

**_Rationale behind this:-_**  
By allowing Data to accumulate in large batches before delivery, we're **_reducing the number of PUT requests to S3_**, thereby reducing transaction costs. This also results in **_improvising the throughput_** through batching and subsequent storage. Something around **_300-600 seconds_** would be a good number to start with.

Buffer Size has been maximised, Costs would be lowered, but **_at the cost of a higher latency_**. 

Cranking up the Buffer Interval to **_900 seconds_** (max possible) would be a relative choice.  
***Point to Note:-- We need to strike balance between the **timely availability of data versus the operational costs** incurred.****

</br>

### _Snappy Compression  Encryption for S3 -_

&#8594; I've utilized **_Snappy compression_** for source records

> Why did we compress the records? Its equal to faster transmission plus cost savings in storage. I'm prioritising **_high speed over a higher compression ratio*_*.
 
&#8594; **_Encryption_** is implemented through **_AWS-owned keys_** for security and confidentiality of data as it moves through the Firehose stream, particularly crucial when converting data formats like JSON to Parquet.

</br>

## ***Workflow #2:- Stream Processing & Visualisation**

The **key service we've used here is Kinesis Data Analytics (KDA)**

</br>

‚öôÔ∏è This is **Workflow #2**     

As we've mentioned, data is ingested through KDS in the form of JSON Blobs. 

We'll use a **Flink Application** deployed on  **Kinesis Data Analytics**. 

</br>

> **I used Flink over SQL.** Why? Because Flink excels at **extracting real-time insights from streaming data**.
>    
>So when my equation is = **Huge Volumes + Huge Velocity**, the answer has to be Flink    
> Also, **whenever we're encountered with eventful processing**, some complex computations, Flink wins over SQL

</br>

### Why did we incorporate OpenSearch alongside Flink?

Flink is awesome for _real-time data processing_

> ‚û°Ô∏è **This means it'll help us in performing some complex computations, _as data flows through the system_**

However, once we're done with processing, **OpenSearch will be our search and analytics engine                                                 
--> It helps us in _actually extracting useful insights from the processed data + some data visualisation capabilities_** üëç

</br>

## How does the workflow actually look like?

 **We've defined a Kinesis Connector for Flink** to read from the Stream            
   &nbsp;    **‚Üì**            
**And an OpenSearch Connector** to write processed data to OpenSearch            
   &nbsp;     **‚Üì**                        
**We've then created the taxi_trips table in Flink** and **then linked it to the Kinesis stream** --> _virtual data mapping_                        
   &nbsp;     ‚Üì            
**Our trip_statistics table in OpenSearch will store aggregated data** --> trip counts and average duration            
   &nbsp;     ‚Üì                        
**We'll then execute some analytical queries** --> Insights into some critical metrics                                    
   &nbsp;     ‚Üì            
Finally, **some data aggregation & visualization with summarized data**            


</br>

## Wrapping it Up

Thank you so much for accompanying me on my journey!

I'll quickly summarise all that we've built today:-

### **Workflow - 1 :- Data Ingestion to Storage**

   Ingested data via Kinesis Data Streams     
         ‚¨á      
   Transferred to S3 via Firehose    
         ‚¨á        
    Managed Schema in Glue       
         ‚¨á      
   Enriched plus standardised data via Lambda     
         ‚¨á    
   Stored in S3

 </br>
 
### **Workflow 2:- Stream Processing and Visualization**    

Flink Application on KDA Studio for real-time processing    
                ‚¨á     
Aggregating the data     
                ‚¨á    
S3 as Durable Data Store   
                ‚¨á   
Visualising the data with OpenSearch     



## Acknowledgements & Attributions

Really appreciate AWS Workshops for their resources and support in this project's development.
 [AWS Workshops](https://www.aws.training/learningobject/curriculum?id=20685).








    

